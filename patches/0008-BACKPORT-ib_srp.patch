From: Israel Rukshin <israelr@mellanox.com>
Subject: [PATCH] BACKPORT: ib_srp

Change-Id: Ic90bc43f6bd61818530da7fb700962a8e1ef4aa5
Signed-off-by: Israel Rukshin <israelr@mellanox.com>
---
 drivers/infiniband/ulp/srp/ib_srp.c | 166 +++++++++++++++++++++++++++++++++++-
 drivers/infiniband/ulp/srp/ib_srp.h |  24 ++++++
 2 files changed, 186 insertions(+), 4 deletions(-)

diff --git a/drivers/infiniband/ulp/srp/ib_srp.c b/drivers/infiniband/ulp/srp/ib_srp.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/ulp/srp/ib_srp.c
+++ b/drivers/infiniband/ulp/srp/ib_srp.c
@@ -30,6 +30,9 @@
  * SOFTWARE.
  */
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 #include <linux/module.h>
@@ -81,8 +84,13 @@ MODULE_PARM_DESC(cmd_sg_entries,
 		 "Default number of gather/scatter entries in the SRP command (default is 12, max 255)");
 
 module_param(indirect_sg_entries, uint, 0444);
+#ifdef HAVE_SG_MAX_SEGMENTS
 MODULE_PARM_DESC(indirect_sg_entries,
 		 "Default max number of gather/scatter entries (default is 12, max is " __stringify(SG_MAX_SEGMENTS) ")");
+#else
+MODULE_PARM_DESC(indirect_sg_entries,
+		 "Default max number of gather/scatter entries (default is 12, max is " __stringify(SCSI_MAX_SG_CHAIN_SEGMENTS) ")");
+#endif
 
 module_param(allow_ext_sg, bool, 0444);
 MODULE_PARM_DESC(allow_ext_sg,
@@ -105,7 +113,7 @@ MODULE_PARM_DESC(never_register, "Never register memory");
 
 static const struct kernel_param_ops srp_tmo_ops;
 
-static int srp_reconnect_delay = 10;
+static int srp_reconnect_delay = 20;
 module_param_cb(reconnect_delay, &srp_tmo_ops, &srp_reconnect_delay,
 		S_IRUGO | S_IWUSR);
 MODULE_PARM_DESC(reconnect_delay, "Time between successive reconnect attempts");
@@ -758,7 +766,7 @@ static int srp_send_req(struct srp_rdma_ch *ch, bool multich)
 		shost_printk(KERN_DEBUG, target->scsi_host,
 			     PFX "Topspin/Cisco initiator port ID workaround "
 			     "activated for target GUID %016llx\n",
-			     be64_to_cpu(target->ioc_guid));
+			     (unsigned long long) be64_to_cpu(target->ioc_guid));
 		memset(req->priv.initiator_port_id, 0, 8);
 		memcpy(req->priv.initiator_port_id + 8,
 		       &target->srp_host->srp_dev->dev->node_guid, 8);
@@ -846,6 +854,9 @@ static int srp_alloc_req_data(struct srp_rdma_ch *ch)
 	dma_addr_t dma_addr;
 	int i, ret = -ENOMEM;
 
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	INIT_LIST_HEAD(&ch->free_reqs);
+#endif
 	ch->req_ring = kcalloc(target->req_ring_size, sizeof(*ch->req_ring),
 			       GFP_KERNEL);
 	if (!ch->req_ring)
@@ -877,6 +888,10 @@ static int srp_alloc_req_data(struct srp_rdma_ch *ch)
 			goto out;
 
 		req->indirect_dma_addr = dma_addr;
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+		req->index = i;
+		list_add_tail(&req->list, &ch->free_reqs);
+#endif
 	}
 	ret = 0;
 
@@ -1130,6 +1145,9 @@ static void srp_free_req(struct srp_rdma_ch *ch, struct srp_request *req,
 
 	spin_lock_irqsave(&ch->lock, flags);
 	ch->req_lim += req_lim_delta;
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	list_add_tail(&req->list, &ch->free_reqs);
+#endif
 	spin_unlock_irqrestore(&ch->lock, flags);
 }
 
@@ -1874,11 +1892,18 @@ static void srp_process_rsp(struct srp_rdma_ch *ch, struct srp_rsp *rsp)
 			ch->tsk_mgmt_status = rsp->data[3];
 		complete(&ch->tsk_mgmt_done);
 	} else {
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 		scmnd = scsi_host_find_tag(target->scsi_host, rsp->tag);
-		if (scmnd) {
+		if (scmnd && scmnd->host_scribble) {
 			req = (void *)scmnd->host_scribble;
 			scmnd = srp_claim_req(ch, req, NULL, scmnd);
+		} else {
+			scmnd = NULL;
 		}
+#else
+		req = &ch->req_ring[rsp->tag];
+		scmnd = srp_claim_req(ch, req, NULL, NULL);
+#endif
 		if (!scmnd) {
 			shost_printk(KERN_ERR, target->scsi_host,
 				     "Null scmnd for RSP w/tag %#016llx received on ch %td / QP %#x\n",
@@ -1974,8 +1999,13 @@ static void srp_process_aer_req(struct srp_rdma_ch *ch,
 	};
 	s32 delta = be32_to_cpu(req->req_lim_delta);
 
+#ifdef HAVE_SCSI_DEVICE_U64_LUN
 	shost_printk(KERN_ERR, target->scsi_host, PFX
-		     "ignoring AER for LUN %llu\n", scsilun_to_int(&req->lun));
+		     "ignoring AER for LUN %u\n", scsilun_to_int(&req->lun));
+#else
+	shost_printk(KERN_ERR, target->scsi_host, PFX
+		     "ignoring AER for LUN %u\n", scsilun_to_int(&req->lun));
+#endif
 
 	if (srp_response_common(ch, delta, &rsp, sizeof(rsp)))
 		shost_printk(KERN_ERR, target->scsi_host, PFX
@@ -2085,7 +2115,9 @@ static int srp_queuecommand(struct Scsi_Host *shost, struct scsi_cmnd *scmnd)
 	struct ib_device *dev;
 	unsigned long flags;
 	u32 tag;
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	u16 idx;
+#endif
 	int len, ret;
 	const bool in_scsi_eh = !in_interrupt() && current == shost->ehandler;
 
@@ -2102,6 +2134,7 @@ static int srp_queuecommand(struct Scsi_Host *shost, struct scsi_cmnd *scmnd)
 	if (unlikely(scmnd->result))
 		goto err;
 
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	WARN_ON_ONCE(scmnd->request->tag < 0);
 	tag = blk_mq_unique_tag(scmnd->request);
 	ch = &target->ch[blk_mq_unique_tag_to_hwq(tag)];
@@ -2109,15 +2142,28 @@ static int srp_queuecommand(struct Scsi_Host *shost, struct scsi_cmnd *scmnd)
 	WARN_ONCE(idx >= target->req_ring_size, "%s: tag %#x: idx %d >= %d\n",
 		  dev_name(&shost->shost_gendev), tag, idx,
 		  target->req_ring_size);
+#else
+	ch = &target->ch[0];
+#endif
 
 	spin_lock_irqsave(&ch->lock, flags);
 	iu = __srp_get_tx_iu(ch, SRP_IU_CMD);
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	spin_unlock_irqrestore(&ch->lock, flags);
 
 	if (!iu)
 		goto err;
 
 	req = &ch->req_ring[idx];
+#else
+	if (!iu)
+		goto err_unlock;
+
+	req = list_first_entry(&ch->free_reqs, struct srp_request, list);
+	list_del(&req->list);
+	tag = req->index;
+	spin_unlock_irqrestore(&ch->lock, flags);
+#endif
 	dev = target->srp_host->srp_dev->dev;
 	ib_dma_sync_single_for_cpu(dev, iu->dma, target->max_iu_len,
 				   DMA_TO_DEVICE);
@@ -2178,6 +2224,14 @@ err_iu:
 	 */
 	req->scmnd = NULL;
 
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	spin_lock_irqsave(&ch->lock, flags);
+	list_add(&req->list, &ch->free_reqs);
+
+err_unlock:
+	spin_unlock_irqrestore(&ch->lock, flags);
+
+#endif
 err:
 	if (scmnd->result) {
 		scmnd->scsi_done(scmnd);
@@ -2493,6 +2547,31 @@ static int srp_cm_handler(struct ib_cm_id *cm_id, struct ib_cm_event *event)
 	return 0;
 }
 
+#if defined(HAVE_SCSI_HOST_TEMPLATE_CHANGE_QUEUE_TYPE) && \
+	!defined(HAVE_SCSI_TCQ_SCSI_CHANGE_QUEUE_TYPE)
+/**
+ * srp_change_queue_type - changing device queue tag type
+ * @sdev: scsi device struct
+ * @tag_type: requested tag type
+ *
+ * Returns queue tag type.
+ */
+static int
+srp_change_queue_type(struct scsi_device *sdev, int tag_type)
+{
+	if (sdev->tagged_supported) {
+		scsi_set_tag_type(sdev, tag_type);
+		if (tag_type)
+			scsi_activate_tcq(sdev, sdev->queue_depth);
+		else
+			scsi_deactivate_tcq(sdev, sdev->queue_depth);
+	} else
+		tag_type = 0;
+
+	return tag_type;
+}
+#endif
+
 /**
  * srp_change_queue_depth - setting device queue depth
  * @sdev: scsi device struct
@@ -2500,13 +2579,40 @@ static int srp_cm_handler(struct ib_cm_id *cm_id, struct ib_cm_event *event)
  *
  * Returns queue depth.
  */
+#ifdef HAVE_SCSI_HOST_TEMPLATE_TRACK_QUEUE_DEPTH
 static int
 srp_change_queue_depth(struct scsi_device *sdev, int qdepth)
 {
 	if (!sdev->tagged_supported)
 		qdepth = 1;
+#ifdef HAVE_SCSI_CHANGE_QUEUE_DEPTH
 	return scsi_change_queue_depth(sdev, qdepth);
+#else
+	scsi_adjust_queue_depth(sdev, qdepth);
+	return sdev->queue_depth;
+#endif //HAVE_SCSI_CHANGE_QUEUE_DEPTH
+}
+#else
+static int
+srp_change_queue_depth(struct scsi_device *sdev, int qdepth, int reason)
+{
+	struct Scsi_Host *shost = sdev->host;
+	int max_depth;
+	if (reason == SCSI_QDEPTH_DEFAULT || reason == SCSI_QDEPTH_RAMP_UP) {
+		max_depth = shost->can_queue;
+		if (!sdev->tagged_supported)
+			max_depth = 1;
+		if (qdepth > max_depth)
+			qdepth = max_depth;
+		scsi_adjust_queue_depth(sdev, scsi_get_tag_type(sdev), qdepth);
+	} else if (reason == SCSI_QDEPTH_QFULL)
+		scsi_track_queue_full(sdev, qdepth);
+	else
+		return -EOPNOTSUPP;
+
+	return sdev->queue_depth;
 }
+#endif //HAVE_SCSI_HOST_TEMPLATE_TRACK_QUEUE_DEPTH
 
 static int srp_send_tsk_mgmt(struct srp_rdma_ch *ch, u64 req_tag, u64 lun,
 			     u8 func)
@@ -2578,8 +2684,13 @@ static int srp_abort(struct scsi_cmnd *scmnd)
 
 	if (!req)
 		return SUCCESS;
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	tag = blk_mq_unique_tag(scmnd->request);
 	ch_idx = blk_mq_unique_tag_to_hwq(tag);
+#else
+	tag = req->index;
+	ch_idx = srp_tag_ch(tag);
+#endif
 	if (WARN_ON_ONCE(ch_idx >= target->ch_count))
 		return SUCCESS;
 	ch = &target->ch[ch_idx];
@@ -2637,6 +2748,7 @@ static int srp_reset_host(struct scsi_cmnd *scmnd)
 	return srp_reconnect_rport(target->rport) == 0 ? SUCCESS : FAILED;
 }
 
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 static int srp_slave_alloc(struct scsi_device *sdev)
 {
 	struct Scsi_Host *shost = sdev->host;
@@ -2650,6 +2762,7 @@ static int srp_slave_alloc(struct scsi_device *sdev)
 
 	return 0;
 }
+#endif
 
 static int srp_slave_configure(struct scsi_device *sdev)
 {
@@ -2842,11 +2955,20 @@ static struct scsi_host_template srp_template = {
 	.module				= THIS_MODULE,
 	.name				= "InfiniBand SRP initiator",
 	.proc_name			= DRV_NAME,
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 	.slave_alloc			= srp_slave_alloc,
+#endif
 	.slave_configure		= srp_slave_configure,
 	.info				= srp_target_info,
 	.queuecommand			= srp_queuecommand,
 	.change_queue_depth             = srp_change_queue_depth,
+#ifdef HAVE_SCSI_HOST_TEMPLATE_CHANGE_QUEUE_TYPE
+#ifdef HAVE_SCSI_TCQ_SCSI_CHANGE_QUEUE_TYPE
+	.change_queue_type		= scsi_change_queue_type,
+#else
+	.change_queue_type		= srp_change_queue_type,
+#endif
+#endif
 	.eh_abort_handler		= srp_abort,
 	.eh_device_reset_handler	= srp_reset_device,
 	.eh_host_reset_handler		= srp_reset_host,
@@ -2857,7 +2979,15 @@ static struct scsi_host_template srp_template = {
 	.cmd_per_lun			= SRP_DEFAULT_CMD_SQ_SIZE,
 	.use_clustering			= ENABLE_CLUSTERING,
 	.shost_attrs			= srp_host_attrs,
+#ifdef HAVE_SCSI_HOST_TEMPLATE_USE_HOST_WIDE_TAGS
+	.use_host_wide_tags		= 1,
+#endif
+#ifdef HAVE_SCSI_HOST_TEMPLATE_USE_BLK_TAGS
+	.use_blk_tags			= 1,
+#endif
+#ifdef HAVE_SCSI_HOST_TEMPLATE_TRACK_QUEUE_DEPTH
 	.track_queue_depth		= 1,
+#endif
 };
 
 static int srp_sdev_count(struct Scsi_Host *host)
@@ -2906,8 +3036,13 @@ static int srp_add_target(struct srp_host *host, struct srp_target_port *target)
 	list_add_tail(&target->list, &host->target_list);
 	spin_unlock(&host->target_lock);
 
+#ifdef HAVE_SCSI_SCAN_INITIAL
 	scsi_scan_target(&target->scsi_host->shost_gendev,
 			 0, target->scsi_id, SCAN_WILD_CARD, SCSI_SCAN_INITIAL);
+#else
+        scsi_scan_target(&target->scsi_host->shost_gendev,
+                         0, target->scsi_id, SCAN_WILD_CARD, 0);
+#endif
 
 	if (srp_connected_ch(target) < target->ch_count ||
 	    target->qp_in_error) {
@@ -3184,12 +3319,21 @@ static int srp_parse_options(const char *buf, struct srp_target_port *target)
 			break;
 
 		case SRP_OPT_SG_TABLESIZE:
+#ifdef HAVE_SG_MAX_SEGMENTS
 			if (match_int(args, &token) || token < 1 ||
 					token > SG_MAX_SEGMENTS) {
 				pr_warn("bad max sg_tablesize parameter '%s'\n",
 					p);
 				goto out;
 			}
+#else
+			if (match_int(args, &token) || token < 1 ||
+					token > SCSI_MAX_SG_CHAIN_SEGMENTS) {
+				pr_warn("bad max sg_tablesize parameter '%s'\n",
+					p);
+				goto out;
+			}
+#endif
 			target->sg_tablesize = token;
 			break;
 
@@ -3257,6 +3401,11 @@ static ssize_t srp_create_target(struct device *dev,
 	if (!target_host)
 		return -ENOMEM;
 
+#if defined(HAVE_SCSI_HOST_USE_BLK_MQ) && \
+	defined(HAVE_SCSI_TCQ_SCSI_INIT_SHARED_TAG_MAP) && \
+	!defined(HAVE_SCSI_HOST_TEMPLATE_USE_BLK_TAGS)
+	target_host->use_blk_mq = true;
+#endif
 	target_host->transportt  = ib_srp_transport_template;
 	target_host->max_channel = 0;
 	target_host->max_id      = 1;
@@ -3288,6 +3437,12 @@ static ssize_t srp_create_target(struct device *dev,
 	if (ret)
 		goto out;
 
+#ifdef HAVE_SCSI_HOST_TEMPLATE_USE_BLK_TAGS
+	ret = scsi_init_shared_tag_map(target_host, target_host->can_queue);
+	if (ret)
+		goto out;
+#endif
+
 	target->req_ring_size = target->queue_size - SRP_TSK_MGMT_SQ_SIZE;
 
 	if (!srp_conn_unique(target->srp_host, target)) {
@@ -3319,6 +3474,7 @@ static ssize_t srp_create_target(struct device *dev,
 		 * register_always is true. Hence add one to mr_per_cmd if
 		 * register_always has been set.
 		 */
+
 		max_sectors_per_mr = srp_dev->max_pages_per_mr <<
 				  (ilog2(srp_dev->mr_page_size) - 9);
 		mr_per_cmd = register_always +
@@ -3417,7 +3573,9 @@ static ssize_t srp_create_target(struct device *dev,
 	}
 
 connected:
+#ifdef HAVE_SCSI_HOST_NR_HW_QUEUES
 	target->scsi_host->nr_hw_queues = target->ch_count;
+#endif
 
 	ret = srp_add_target(host, target);
 	if (ret)
diff --git a/drivers/infiniband/ulp/srp/ib_srp.h b/drivers/infiniband/ulp/srp/ib_srp.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/ulp/srp/ib_srp.h
+++ b/drivers/infiniband/ulp/srp/ib_srp.h
@@ -80,6 +80,23 @@ enum srp_iu_type {
 	SRP_IU_RSP,
 };
 
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+static inline u32 build_srp_tag(u16 ch, u16 req_idx)
+{
+	return ch << 16 | req_idx;
+}
+
+static inline u16 srp_tag_ch(u32 tag)
+{
+	return tag >> 16;
+}
+
+static inline u16 srp_tag_idx(u32 tag)
+{
+	return tag & ((1 << 16) - 1);
+}
+#endif
+
 /*
  * @mr_page_mask: HCA memory registration page mask.
  * @mr_page_size: HCA memory registration page size.
@@ -113,6 +130,10 @@ struct srp_host {
 };
 
 struct srp_request {
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	struct list_head        list;
+	short                   index;
+#endif
 	struct scsi_cmnd       *scmnd;
 	struct srp_iu	       *cmd;
 	union {
@@ -133,6 +154,9 @@ struct srp_request {
 struct srp_rdma_ch {
 	/* These are RW in the hot path, and commonly used together */
 	struct list_head	free_tx;
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	struct list_head        free_reqs;
+#endif
 	spinlock_t		lock;
 	s32			req_lim;
 
