From: Vladimir Neyelov <vladimirn@mellanox.com>
Subject: [PATCH] BACKPORT: ib_iser

Change-Id: Ib85ca82399e22f4c6d053b764ce79ff4aefb25d9
Signed-off-by: Vladimir Neyelov <vladimirn@mellanox.com>
---
 drivers/infiniband/ulp/iser/iscsi_iser.c     |  37 ++-
 drivers/infiniband/ulp/iser/iscsi_iser.h     |   9 +
 drivers/infiniband/ulp/iser/iser_initiator.c |  73 +++++-
 drivers/infiniband/ulp/iser/iser_memory.c    | 364 ++++++++++++++++++++++++++-
 drivers/infiniband/ulp/iser/iser_verbs.c     |   2 +
 5 files changed, 478 insertions(+), 7 deletions(-)

diff --git a/drivers/infiniband/ulp/iser/iscsi_iser.c b/drivers/infiniband/ulp/iser/iscsi_iser.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/ulp/iser/iscsi_iser.c
+++ b/drivers/infiniband/ulp/iser/iscsi_iser.c
@@ -108,7 +108,11 @@ MODULE_PARM_DESC(pi_enable, "Enable T10-PI offload support (default:disabled)");
 
 int iser_pi_guard;
 module_param_named(pi_guard, iser_pi_guard, int, S_IRUGO);
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 MODULE_PARM_DESC(pi_guard, "T10-PI guard_type [deprecated]");
+#else
+MODULE_PARM_DESC(pi_guard, "T10-PI guard_type, 0:CRC|1:IP_CSUM (default:IP_CSUM)");
+#endif
 
 /*
  * iscsi_iser_recv() - Process a successful recv completion
@@ -390,6 +394,7 @@ static void iscsi_iser_cleanup_task(struct iscsi_task *task)
 	}
 }
 
+#ifdef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
 /**
  * iscsi_iser_check_protection() - check protection information status of task.
  * @task:     iscsi task
@@ -414,6 +419,7 @@ iscsi_iser_check_protection(struct iscsi_task *task, sector_t *sector)
 		return iser_check_task_pi_status(iser_task, ISER_DIR_OUT,
 						 sector);
 }
+#endif
 
 /**
  * iscsi_iser_conn_create() - create a new iscsi-iser connection
@@ -769,7 +775,13 @@ iscsi_iser_conn_get_stats(struct iscsi_cls_conn *cls_conn, struct iscsi_stats *s
 	stats->r2t_pdus = conn->r2t_pdus_cnt; /* always 0 */
 	stats->tmfcmd_pdus = conn->tmfcmd_pdus_cnt;
 	stats->tmfrsp_pdus = conn->tmfrsp_pdus_cnt;
+#if defined(HAVE_QUEUE_FLAG_SG_GAPS) || defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
 	stats->custom_length = 0;
+#else
+	stats->custom_length = 1;
+        strcpy(stats->custom[0].desc, "fmr_unalign_cnt");
+        stats->custom[0].value = conn->fmr_unalign_cnt;
+#endif
 }
 
 static int iscsi_iser_get_ep_param(struct iscsi_endpoint *ep,
@@ -964,7 +976,9 @@ static umode_t iser_attr_is_visible(int param_type, int param)
 		case ISCSI_PARAM_TGT_RESET_TMO:
 		case ISCSI_PARAM_IFACE_NAME:
 		case ISCSI_PARAM_INITIATOR_NAME:
+#ifdef HAVE_ISCSI_PARAM_DISCOVERY_SESS
 		case ISCSI_PARAM_DISCOVERY_SESS:
+#endif
 			return S_IRUGO;
 		default:
 			return 0;
@@ -974,6 +988,7 @@ static umode_t iser_attr_is_visible(int param_type, int param)
 	return 0;
 }
 
+#if defined(HAVE_QUEUE_FLAG_SG_GAPS) || defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
 static int iscsi_iser_slave_alloc(struct scsi_device *sdev)
 {
 	struct iscsi_session *session;
@@ -985,16 +1000,26 @@ static int iscsi_iser_slave_alloc(struct scsi_device *sdev)
 	ib_dev = iser_conn->ib_conn.device->ib_device;
 
 	if (!(ib_dev->attrs.device_cap_flags & IB_DEVICE_SG_GAPS_REG))
+	{
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 		blk_queue_virt_boundary(sdev->request_queue, ~MASK_4K);
-
+#else
+		queue_flag_set_unlocked(QUEUE_FLAG_SG_GAPS, sdev->request_queue);
+#endif
+	}
 	return 0;
 }
+#endif
 
 static struct scsi_host_template iscsi_iser_sht = {
 	.module                 = THIS_MODULE,
 	.name                   = "iSCSI Initiator over iSER",
 	.queuecommand           = iscsi_queuecommand,
+#ifdef HAVE_SCSI_CHANGE_QUEUE_DEPTH
 	.change_queue_depth	= scsi_change_queue_depth,
+#else
+	.change_queue_depth	= iscsi_change_queue_depth,
+#endif
 	.sg_tablesize           = ISCSI_ISER_DEF_SG_TABLESIZE,
 	.cmd_per_lun            = ISER_DEF_CMD_PER_LUN,
 	.eh_abort_handler       = iscsi_eh_abort,
@@ -1002,16 +1027,24 @@ static struct scsi_host_template iscsi_iser_sht = {
 	.eh_target_reset_handler = iscsi_eh_recover_target,
 	.target_alloc		= iscsi_target_alloc,
 	.use_clustering         = ENABLE_CLUSTERING,
+#if defined(HAVE_QUEUE_FLAG_SG_GAPS) || defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
 	.slave_alloc            = iscsi_iser_slave_alloc,
+#endif
 	.proc_name              = "iscsi_iser",
 	.this_id                = -1,
+#ifdef HAVE_SCSI_HOST_TEMPLATE_TRACK_QUEUE_DEPTH
 	.track_queue_depth	= 1,
+#endif
 };
 
 static struct iscsi_transport iscsi_iser_transport = {
 	.owner                  = THIS_MODULE,
 	.name                   = "iser",
+#ifdef HAVE_DISCOVERY_SESSION
 	.caps                   = CAP_RECOVERY_L0 | CAP_MULTI_R2T | CAP_TEXT_NEGO,
+#else
+	.caps                   = CAP_RECOVERY_L0 | CAP_MULTI_R2T,
+#endif
 	/* session management */
 	.create_session         = iscsi_iser_session_create,
 	.destroy_session        = iscsi_iser_session_destroy,
@@ -1036,7 +1069,9 @@ static struct iscsi_transport iscsi_iser_transport = {
 	.xmit_task		= iscsi_iser_task_xmit,
 	.cleanup_task		= iscsi_iser_cleanup_task,
 	.alloc_pdu		= iscsi_iser_pdu_alloc,
+#ifdef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
 	.check_protection	= iscsi_iser_check_protection,
+#endif
 	/* recovery */
 	.session_recovery_timedout = iscsi_session_recovery_timedout,
 
diff --git a/drivers/infiniband/ulp/iser/iscsi_iser.h b/drivers/infiniband/ulp/iser/iscsi_iser.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/ulp/iser/iscsi_iser.h
+++ b/drivers/infiniband/ulp/iser/iscsi_iser.h
@@ -68,6 +68,11 @@
 #include <rdma/ib_fmr_pool.h>
 #include <rdma/rdma_cm.h>
 
+#if defined(CONFIG_COMPAT_RHEL_7_3) || defined(CONFIG_COMPAT_RHEL_7_2)
+	#undef HAVE_QUEUE_FLAG_SG_GAPS
+	#undef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+#endif
+
 #define DRV_NAME	"iser"
 #define PFX		DRV_NAME ": "
 #define DRV_VER		"1.6"
@@ -198,6 +203,10 @@ struct iser_data_buf {
 	int                size;
 	unsigned long      data_len;
 	unsigned int       dma_nents;
+#if !defined(HAVE_QUEUE_FLAG_SG_GAPS) && !defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
+	struct scatterlist *orig_sg;
+	unsigned int       orig_size;
+#endif
 };
 
 /* fwd declarations */
diff --git a/drivers/infiniband/ulp/iser/iser_initiator.c b/drivers/infiniband/ulp/iser/iser_initiator.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/ulp/iser/iser_initiator.c
+++ b/drivers/infiniband/ulp/iser/iser_initiator.c
@@ -322,7 +322,9 @@ static int iser_post_rx_bufs(struct iscsi_conn *conn, struct iscsi_hdr *req)
 {
 	struct iser_conn *iser_conn = conn->dd_data;
 	struct ib_conn *ib_conn = &iser_conn->ib_conn;
+#ifdef HAVE_DISCOVERY_SESSION
 	struct iscsi_session *session = conn->session;
+#endif
 
 	iser_dbg("req op %x flags %x\n", req->opcode, req->flags);
 	/* check if this is the last login - going to full feature phase */
@@ -334,14 +336,15 @@ static int iser_post_rx_bufs(struct iscsi_conn *conn, struct iscsi_hdr *req)
 	 * (for the last login response).
 	 */
 	WARN_ON(ib_conn->post_recv_buf_count != 1);
-
+	
+#ifdef HAVE_DISCOVERY_SESSION
 	if (session->discovery_sess) {
 		iser_info("Discovery session, re-using login RX buffer\n");
 		return 0;
 	} else
 		iser_info("Normal session, posting batch of RX %d buffers\n",
 			  iser_conn->min_posted_rx);
-
+#endif
 	/* Initial post receive buffers */
 	if (iser_post_recvm(iser_conn, iser_conn->min_posted_rx))
 		return -ENOMEM;
@@ -365,7 +368,11 @@ int iser_send_command(struct iscsi_conn *conn,
 	unsigned long edtl;
 	int err;
 	struct iser_data_buf *data_buf, *prot_buf;
+#ifdef HAVE_ISCSI_CMD
+	struct iscsi_cmd *hdr = (struct iscsi_cmd *)task->hdr;
+#else	
 	struct iscsi_scsi_req *hdr = (struct iscsi_scsi_req *)task->hdr;
+#endif
 	struct scsi_cmnd *sc  =  task->sc;
 	struct iser_tx_desc *tx_desc = &iser_task->desc;
 	u8 sig_count = ++iser_conn->ib_conn.sig_count;
@@ -752,7 +759,11 @@ void iser_task_rdma_init(struct iscsi_iser_task *iser_task)
 void iser_task_rdma_finalize(struct iscsi_iser_task *iser_task)
 {
 	int prot_count = scsi_prot_sg_count(iser_task->sc);
-
+#if !defined(HAVE_QUEUE_FLAG_SG_GAPS) && !defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
+	int is_rdma_data_aligned = 1;
+	int is_rdma_prot_aligned = 1;
+#endif
+#if defined(HAVE_QUEUE_FLAG_SG_GAPS) || defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
 	if (iser_task->dir[ISER_DIR_IN]) {
 		iser_unreg_rdma_mem(iser_task, ISER_DIR_IN);
 		iser_dma_unmap_task_data(iser_task,
@@ -774,4 +785,60 @@ void iser_task_rdma_finalize(struct iscsi_iser_task *iser_task)
 						 &iser_task->prot[ISER_DIR_OUT],
 						 DMA_TO_DEVICE);
 	}
+#else
+	/* if we were reading, copy back to unaligned sglist,
+	* anyway dma_unmap and free the copy
+	*/
+	if (iser_task->data[ISER_DIR_IN].orig_sg) {
+		is_rdma_data_aligned = 0;
+		iser_finalize_rdma_unaligned_sg(iser_task,
+						&iser_task->data[ISER_DIR_IN],
+						ISER_DIR_IN);
+	}
+
+	if (iser_task->data[ISER_DIR_OUT].orig_sg) {
+		is_rdma_data_aligned = 0;
+		iser_finalize_rdma_unaligned_sg(iser_task,
+						&iser_task->data[ISER_DIR_OUT],
+						ISER_DIR_OUT);
+	}
+
+	if (iser_task->prot[ISER_DIR_IN].orig_sg) {
+		is_rdma_prot_aligned = 0;
+		iser_finalize_rdma_unaligned_sg(iser_task,
+						&iser_task->prot[ISER_DIR_IN],
+						ISER_DIR_IN);
+	}
+
+	if (iser_task->prot[ISER_DIR_OUT].orig_sg) {
+		is_rdma_prot_aligned = 0;
+		iser_finalize_rdma_unaligned_sg(iser_task,
+						&iser_task->prot[ISER_DIR_OUT],
+						ISER_DIR_OUT);
+	}
+
+	if (iser_task->dir[ISER_DIR_IN]) {
+		iser_unreg_rdma_mem(iser_task, ISER_DIR_IN);
+		if (is_rdma_data_aligned)
+			iser_dma_unmap_task_data(iser_task,
+						&iser_task->data[ISER_DIR_IN],
+						DMA_FROM_DEVICE);
+		if (prot_count && is_rdma_prot_aligned)
+			iser_dma_unmap_task_data(iser_task,
+						&iser_task->prot[ISER_DIR_IN],
+						DMA_FROM_DEVICE);
+	}
+
+	if (iser_task->dir[ISER_DIR_OUT]) {
+		iser_unreg_rdma_mem(iser_task, ISER_DIR_OUT);
+		if (is_rdma_data_aligned)
+			iser_dma_unmap_task_data(iser_task,
+						&iser_task->data[ISER_DIR_OUT],
+						DMA_TO_DEVICE);
+		if (prot_count && is_rdma_prot_aligned)
+			iser_dma_unmap_task_data(iser_task,
+						&iser_task->prot[ISER_DIR_OUT],
+						DMA_TO_DEVICE);
+	}
+#endif
 }
diff --git a/drivers/infiniband/ulp/iser/iser_memory.c b/drivers/infiniband/ulp/iser/iser_memory.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/ulp/iser/iser_memory.c
+++ b/drivers/infiniband/ulp/iser/iser_memory.c
@@ -67,6 +67,258 @@ static const struct iser_reg_ops fmr_ops = {
 	.reg_desc_put	= iser_reg_desc_put_fmr,
 };
 
+#if !defined(HAVE_QUEUE_FLAG_SG_GAPS) && !defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
+#define IS_4K_ALIGNED(addr)    ((((unsigned long)addr) & ~MASK_4K) == 0)
+static void
+iser_free_bounce_sg(struct iser_data_buf *data)
+{
+	struct scatterlist *sg;
+	int count;
+
+	for_each_sg(data->sg, sg, data->size, count)
+		__free_page(sg_page(sg));
+
+	kfree(data->sg);
+
+	data->sg = data->orig_sg;
+	data->size = data->orig_size;
+	data->orig_sg = NULL;
+	data->orig_size = 0;
+}
+
+static int
+iser_alloc_bounce_sg(struct iser_data_buf *data)
+{
+	struct scatterlist *sg;
+	struct page *page;
+	unsigned long length = data->data_len;
+	int i = 0, nents = DIV_ROUND_UP(length, PAGE_SIZE);
+
+	sg = kcalloc(nents, sizeof(*sg), GFP_ATOMIC);
+	if (!sg)
+		goto err;
+
+	sg_init_table(sg, nents);
+	while (length) {
+		u32 page_len = min_t(u32, length, PAGE_SIZE);
+
+		page = alloc_page(GFP_ATOMIC);
+		if (!page)
+			goto err;
+
+		sg_set_page(&sg[i], page, page_len, 0);
+		length -= page_len;
+		i++;
+	}
+
+	data->orig_sg = data->sg;
+	data->orig_size = data->size;
+	data->sg = sg;
+	data->size = nents;
+
+	return 0;
+
+err:
+	for (; i > 0; i--)
+		__free_page(sg_page(&sg[i - 1]));
+	kfree(sg);
+
+	return -ENOMEM;
+}
+
+static void
+iser_copy_bounce(struct iser_data_buf *data, bool to_buffer)
+{
+	struct scatterlist *osg, *bsg = data->sg;
+	void *oaddr, *baddr;
+	unsigned int left = data->data_len;
+	unsigned int bsg_off = 0;
+	int i;
+
+	for_each_sg(data->orig_sg, osg, data->orig_size, i) {
+		unsigned int copy_len, osg_off = 0;
+
+#ifdef HAVE_KM_TYPE
+		if (to_buffer)
+			oaddr = kmap_atomic(sg_page(osg), KM_USER0) + osg->offset;
+		else
+			oaddr = kmap_atomic(sg_page(osg), KM_SOFTIRQ0) + osg->offset;
+#else
+		oaddr = kmap_atomic(sg_page(osg)) + osg->offset;
+#endif
+		copy_len = min(left, osg->length);
+		while (copy_len) {
+			unsigned int len = min(copy_len, bsg->length - bsg_off);
+
+#ifdef HAVE_KM_TYPE
+			if (to_buffer)
+				baddr = kmap_atomic(sg_page(bsg), KM_USER0) + bsg->offset;
+			else
+				baddr = kmap_atomic(sg_page(bsg), KM_SOFTIRQ0) + bsg->offset;
+#else
+			baddr = kmap_atomic(sg_page(bsg)) + bsg->offset;
+#endif
+			if (to_buffer)
+				memcpy(baddr + bsg_off, oaddr + osg_off, len);
+			else
+				memcpy(oaddr + osg_off, baddr + bsg_off, len);
+
+#ifdef HAVE_KM_TYPE
+			if (to_buffer)
+				kunmap_atomic(baddr - bsg->offset, KM_USER0);
+			else
+				kunmap_atomic(baddr - bsg->offset, KM_SOFTIRQ0);
+#else
+			kunmap_atomic(baddr - bsg->offset);
+#endif
+			osg_off += len;
+			bsg_off += len;
+			copy_len -= len;
+
+			if (bsg_off >= bsg->length) {
+				bsg = sg_next(bsg);
+					bsg_off = 0;
+			}
+		}
+#ifdef HAVE_KM_TYPE
+		if (to_buffer)
+			kunmap_atomic(oaddr - osg->offset, KM_USER0);
+		else
+			kunmap_atomic(oaddr - osg->offset, KM_SOFTIRQ0);
+#else
+		kunmap_atomic(oaddr - osg->offset);
+#endif
+		left -= osg_off;
+	}
+}
+
+static inline void
+iser_copy_from_bounce(struct iser_data_buf *data)
+{
+	iser_copy_bounce(data, false);
+}
+
+static inline void
+iser_copy_to_bounce(struct iser_data_buf *data)
+{
+	iser_copy_bounce(data, true);
+}
+
+/**
+	* iser_start_rdma_unaligned_sg
+*/
+static int iser_start_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,
+					struct iser_data_buf *data,
+					enum iser_data_dir cmd_dir)
+{
+	struct ib_device *dev = iser_task->iser_conn->ib_conn.device->ib_device;
+	int rc;
+
+	rc = iser_alloc_bounce_sg(data);
+	if (rc) {
+		iser_err("Failed to allocate bounce for data len %lu\n",
+					data->data_len);
+		return rc;
+	}
+
+	if (cmd_dir == ISER_DIR_OUT)
+		iser_copy_to_bounce(data);
+
+	data->dma_nents = ib_dma_map_sg(dev, data->sg, data->size,
+					(cmd_dir == ISER_DIR_OUT) ?
+					DMA_TO_DEVICE : DMA_FROM_DEVICE);
+	if (!data->dma_nents) {
+		iser_err("Got dma_nents %d, something went wrong...\n",
+						data->dma_nents);
+						rc = -ENOMEM;
+		goto err;
+	}
+
+	return 0;
+err:
+	iser_free_bounce_sg(data);
+	return rc;
+}
+
+/**
+ * iser_finalize_rdma_unaligned_sg
+ */
+
+void iser_finalize_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,
+				      struct iser_data_buf *data,
+				      enum iser_data_dir cmd_dir)
+{
+	struct ib_device *dev = iser_task->iser_conn->ib_conn.device->ib_device;
+
+	ib_dma_unmap_sg(dev, data->sg, data->size,
+		 	(cmd_dir == ISER_DIR_OUT) ?
+			DMA_TO_DEVICE : DMA_FROM_DEVICE);
+
+	if (cmd_dir == ISER_DIR_IN)
+		iser_copy_from_bounce(data);
+
+	iser_free_bounce_sg(data);
+}
+
+/**
+ * iser_data_buf_aligned_len - Tries to determine the maximal correctly aligned
+ * for RDMA sub-list of a scatter-gather list of memory buffers, and  returns
+ * the number of entries which are aligned correctly. Supports the case where
+ * consecutive SG elements are actually fragments of the same physcial page.
+ */
+static int iser_data_buf_aligned_len(struct iser_data_buf *data,
+			 	      struct ib_device *ibdev,
+				      unsigned sg_tablesize)
+{
+	struct scatterlist *sg, *sgl, *next_sg = NULL;
+	u64 start_addr, end_addr;
+	int i, ret_len, start_check = 0;
+
+	if (data->dma_nents == 1)
+		return 1;
+
+	sgl = data->sg;
+	start_addr  = ib_sg_dma_address(ibdev, sgl);
+
+	if (unlikely(sgl[0].offset &&
+		data->data_len >= sg_tablesize * PAGE_SIZE)) {
+		iser_dbg("can't register length %lx with offset %x "
+				"fall to bounce buffer\n", data->data_len,
+			sgl[0].offset);
+			return 0;
+	}
+
+	for_each_sg(sgl, sg, data->dma_nents, i) {
+		if (start_check && !IS_4K_ALIGNED(start_addr))
+			break;
+
+		next_sg = sg_next(sg);
+		if (!next_sg)
+			break;
+
+		end_addr    = start_addr + ib_sg_dma_len(ibdev, sg);
+		start_addr  = ib_sg_dma_address(ibdev, next_sg);
+
+		if (end_addr == start_addr) {
+			start_check = 0;
+			continue;
+		} else
+			start_check = 1;
+
+		if (!IS_4K_ALIGNED(end_addr))
+			break;
+	}
+	ret_len = (next_sg) ? i : i+1;
+
+	if (unlikely(ret_len != data->dma_nents))
+		iser_warn("rdma alignment violation (%d/%d aligned)\n",
+					ret_len, data->dma_nents);
+
+	return ret_len;
+}
+
+#endif
+
 void iser_reg_comp(struct ib_cq *cq, struct ib_wc *wc)
 {
 	iser_err_comp(wc, "memreg");
@@ -160,6 +412,54 @@ static void iser_dump_page_vec(struct iser_page_vec *page_vec)
 		iser_err("vec[%d]: %llx\n", i, page_vec->pages[i]);
 }
 
+#if !defined(HAVE_QUEUE_FLAG_SG_GAPS) && !defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
+static int fall_to_bounce_buf(struct iscsi_iser_task *iser_task,
+			       struct iser_data_buf *mem,
+			       enum iser_data_dir cmd_dir)
+{
+	struct iscsi_conn *iscsi_conn = iser_task->iser_conn->iscsi_conn;
+	struct iser_device *device = iser_task->iser_conn->ib_conn.device;
+
+	iscsi_conn->fmr_unalign_cnt++;
+
+	if (iser_debug_level > 0)
+		iser_data_buf_dump(mem, device->ib_device);
+
+	/* unmap the command data before accessing it */
+	iser_dma_unmap_task_data(iser_task, mem,
+				(cmd_dir == ISER_DIR_OUT) ?
+				DMA_TO_DEVICE : DMA_FROM_DEVICE);
+
+	/* allocate copy buf, if we are writing, copy the */
+	/* unaligned scatterlist, dma map the copy        */
+	if (iser_start_rdma_unaligned_sg(iser_task, mem, cmd_dir) != 0)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static int
+iser_handle_unaligned_buf(struct iscsi_iser_task *task,
+			   struct iser_data_buf *mem,
+			   enum iser_data_dir dir)
+{
+	struct iser_conn *iser_conn = task->iser_conn;
+	struct iser_device *device = iser_conn->ib_conn.device;
+	int err, aligned_len;
+
+	aligned_len = iser_data_buf_aligned_len(mem, device->ib_device,
+						iser_conn->scsi_sg_tablesize);
+	if (aligned_len != mem->dma_nents) {
+		err = fall_to_bounce_buf(task, mem, dir);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+#endif 
+
 int iser_dma_map_task_data(struct iscsi_iser_task *iser_task,
 			    struct iser_data_buf *data,
 			    enum iser_data_dir iser_dir,
@@ -306,8 +606,13 @@ iser_set_dif_domain(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs,
 		    struct ib_sig_domain *domain)
 {
 	domain->sig_type = IB_SIG_TYPE_T10_DIF;
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 	domain->sig.dif.pi_interval = scsi_prot_interval(sc);
 	domain->sig.dif.ref_tag = scsi_prot_ref_tag(sc);
+#else
+	domain->sig.dif.pi_interval = sc->device->sector_size;
+	domain->sig.dif.ref_tag = scsi_get_lba(sc) & 0xffffffff;
+#endif
 	/*
 	 * At the moment we hard code those, but in the future
 	 * we will take them from sc.
@@ -315,9 +620,15 @@ iser_set_dif_domain(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs,
 	domain->sig.dif.apptag_check_mask = 0xffff;
 	domain->sig.dif.app_escape = true;
 	domain->sig.dif.ref_escape = true;
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 	if (sc->prot_flags & SCSI_PROT_REF_INCREMENT)
 		domain->sig.dif.ref_remap = true;
-};
+#else
+	if (scsi_get_prot_type(sc) == SCSI_PROT_DIF_TYPE1 ||
+		scsi_get_prot_type(sc) == SCSI_PROT_DIF_TYPE2)
+		domain->sig.dif.ref_remap = true;
+#endif
+}
 
 static int
 iser_set_sig_attrs(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs)
@@ -333,16 +644,26 @@ iser_set_sig_attrs(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs)
 	case SCSI_PROT_WRITE_STRIP:
 		sig_attrs->wire.sig_type = IB_SIG_TYPE_NONE;
 		iser_set_dif_domain(sc, sig_attrs, &sig_attrs->mem);
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 		sig_attrs->mem.sig.dif.bg_type = sc->prot_flags & SCSI_PROT_IP_CHECKSUM ?
 						IB_T10DIF_CSUM : IB_T10DIF_CRC;
+#else
+		sig_attrs->mem.sig.dif.bg_type = iser_pi_guard ? IB_T10DIF_CSUM :
+						IB_T10DIF_CRC;
+#endif
 		break;
 	case SCSI_PROT_READ_PASS:
 	case SCSI_PROT_WRITE_PASS:
 		iser_set_dif_domain(sc, sig_attrs, &sig_attrs->wire);
 		sig_attrs->wire.sig.dif.bg_type = IB_T10DIF_CRC;
 		iser_set_dif_domain(sc, sig_attrs, &sig_attrs->mem);
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 		sig_attrs->mem.sig.dif.bg_type = sc->prot_flags & SCSI_PROT_IP_CHECKSUM ?
 						IB_T10DIF_CSUM : IB_T10DIF_CRC;
+#else
+		sig_attrs->mem.sig.dif.bg_type = iser_pi_guard ? IB_T10DIF_CSUM :
+						IB_T10DIF_CRC;
+#endif
 		break;
 	default:
 		iser_err("Unsupported PI operation %d\n",
@@ -353,6 +674,7 @@ iser_set_sig_attrs(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs)
 	return 0;
 }
 
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 static inline void
 iser_set_prot_checks(struct scsi_cmnd *sc, u8 *mask)
 {
@@ -362,6 +684,30 @@ iser_set_prot_checks(struct scsi_cmnd *sc, u8 *mask)
 	if (sc->prot_flags & SCSI_PROT_GUARD_CHECK)
 		*mask |= ISER_CHECK_GUARD;
 }
+#else
+static int
+iser_set_prot_checks(struct scsi_cmnd *sc, u8 *mask)
+{
+	switch (scsi_get_prot_type(sc)) {
+	case SCSI_PROT_DIF_TYPE0:
+		*mask = 0x0;
+		break;
+	case SCSI_PROT_DIF_TYPE1:
+	case SCSI_PROT_DIF_TYPE2:
+		*mask = ISER_CHECK_GUARD | ISER_CHECK_REFTAG;
+		break;
+	case SCSI_PROT_DIF_TYPE3:
+		*mask = ISER_CHECK_GUARD;
+		break;
+	default:
+		iser_err("Unsupported protection type %d\n",
+			 scsi_get_prot_type(sc));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+#endif
 
 static inline void
 iser_inv_rkey(struct ib_send_wr *inv_wr,
@@ -417,12 +763,14 @@ iser_reg_sig_mr(struct iscsi_iser_task *iser_task,
 			   IB_ACCESS_REMOTE_READ |
 			   IB_ACCESS_REMOTE_WRITE;
 	pi_ctx->sig_mr_valid = 1;
-
 	sig_reg->sge.lkey = mr->lkey;
 	sig_reg->rkey = mr->rkey;
 	sig_reg->sge.addr = 0;
+#ifdef HAVE_SCSI_TRANSFER_LENGTH
 	sig_reg->sge.length = scsi_transfer_length(iser_task->sc);
-
+#else
+	sig_reg->sge.length = scsi_bufflen(iser_task->sc);
+#endif
 	iser_dbg("lkey=0x%x rkey=0x%x addr=0x%llx length=%u\n",
 		 sig_reg->sge.lkey, sig_reg->rkey, sig_reg->sge.addr,
 		 sig_reg->sge.length);
@@ -520,6 +868,11 @@ int iser_reg_rdma_mem(struct iscsi_iser_task *task,
 	bool use_dma_key;
 	int err;
 
+#if !defined(HAVE_QUEUE_FLAG_SG_GAPS) && !defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
+	err = iser_handle_unaligned_buf(task, mem, dir);
+	if (unlikely(err))
+		return err;
+#endif
 	use_dma_key = mem->dma_nents == 1 && (all_imm || !iser_always_reg) &&
 		      scsi_get_prot_op(task->sc) == SCSI_PROT_NORMAL;
 
@@ -542,6 +895,11 @@ int iser_reg_rdma_mem(struct iscsi_iser_task *task,
 
 		if (scsi_prot_sg_count(task->sc)) {
 			mem = &task->prot[dir];
+#if !defined(HAVE_QUEUE_FLAG_SG_GAPS) && !defined(HAVE_BLK_QUEUE_VIRT_BOUNDARY)
+			err = iser_handle_unaligned_buf(task, mem, dir);
+			if (unlikely(err))
+				goto err_reg;
+#endif
 			err = iser_reg_prot_sg(task, mem, desc,
 					       use_dma_key, prot_reg);
 			if (unlikely(err))
diff --git a/drivers/infiniband/ulp/iser/iser_verbs.c b/drivers/infiniband/ulp/iser/iser_verbs.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/ulp/iser/iser_verbs.c
+++ b/drivers/infiniband/ulp/iser/iser_verbs.c
@@ -1109,6 +1109,7 @@ int iser_post_send(struct ib_conn *ib_conn, struct iser_tx_desc *tx_desc,
 	return ib_ret;
 }
 
+#ifdef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
 u8 iser_check_task_pi_status(struct iscsi_iser_task *iser_task,
 			     enum iser_data_dir cmd_dir, sector_t *sector)
 {
@@ -1156,6 +1157,7 @@ err:
 	/* Not alot we can do here, return ambiguous guard error */
 	return 0x1;
 }
+#endif
 
 void iser_err_comp(struct ib_wc *wc, const char *type)
 {
