xeon-phi: add context based umem support

The ib_umem_get routine calls get_user_pages to pin pages and create the
ib_umem structure.  Memory on MIC, however, must be mapped through SCIF for
access across PCI.  This patch allows setup of context-based ib_umem mapping
routines.
---
diff -ruN a3/drivers/infiniband/core/umem.c a4/drivers/infiniband/core/umem.c
--- a3/drivers/infiniband/core/umem.c	2015-09-10 09:31:52.628958359 -0700
+++ a4/drivers/infiniband/core/umem.c	2015-09-10 09:32:16.722901309 -0700
@@ -57,6 +57,10 @@
 	for_each_sg(umem->sg_head.sgl, sg, umem->npages, i) {
 
 		page = sg_page(sg);
+
+		if (!pfn_valid(page_to_pfn(page)))
+			continue;
+
 		if (umem->writable && dirty)
 			set_page_dirty_lock(page);
 		put_page(page);
@@ -68,14 +72,71 @@
 }
 
 /**
- * ib_umem_get - Pin and DMA map userspace memory.
+ * get_remap_pages() - get pages remapped to user virtual space
+ * @mm:		mm struct of target mm
+ * @start:	starting user address
+ * @nr_pages:	number of pages to lookup
+ * @write	flag to verify if vma is writable
+ * @pages:      array that receives pointers to the pages.  Should
+ *              be at least nr_pages long. Or NULL, if caller only
+ *              intends to ensure the pages are valid.
+ * @vmas:       array of pointers to vmas corresponding to each page.
+ *              Or NULL if the caller does not require them.
+ *
+ * Pages may be system ram or io space mmapped to user virtual
+ * space via remap_pfn_range or io_remap_page_range, respectively.
+ *
+ * Returns number of pages found, which may be less than the number
+ * requested.  Returns 0 if nr_pages is 0.
+ *
+ * Must be called with mmap_sem held for read or write.
+ */
+static long get_remap_pages(struct mm_struct *mm, unsigned long start,
+			    unsigned long nr_pages, int write,
+			    struct page **pages, struct vm_area_struct **vmas)
+{
+	struct vm_area_struct *vma;
+	unsigned long pfn;
+	long i = 0;
+	int ret;
+
+	while (nr_pages) {
+		if (!(vma = find_vma(mm, start)))
+			return i ? : -EFAULT;
+		if (write && !(vma->vm_flags & VM_WRITE))
+			return i ? : -EFAULT;
+
+		do {
+			ret = follow_pfn(vma, start, &pfn);
+			if (ret)
+				return i ? : ret;
+
+			if (pages) {
+				pages[i] = pfn_to_page(pfn);
+				if (pfn_valid(pfn))
+					get_page(pages[i]);
+			}
+			if (vmas)
+				vmas[i] = vma;
+
+			start += PAGE_SIZE;
+			nr_pages--;
+			i++;
+		} while (nr_pages && start < vma->vm_end);
+	}
+
+	return i;
+}
+
+/**
+ * ib_get_umem - Pin and DMA map userspace memory.
  * @context: userspace context to pin memory for
  * @addr: userspace virtual address to start at
  * @size: length of region to pin
  * @access: IB_ACCESS_xxx flags for memory being pinned
  * @dmasync: flush in-flight DMA when the memory region is written
  */
-struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,
+struct ib_umem *ib_get_umem(struct ib_ucontext *context, unsigned long addr,
 			    size_t size, int access, int dmasync)
 {
 	struct ib_umem *umem;
@@ -109,7 +170,6 @@
 	if (!umem)
 		return ERR_PTR(-ENOMEM);
 
-	umem->context   = context;
 	umem->length    = size;
 	umem->offset    = addr & ~PAGE_MASK;
 	umem->page_size = PAGE_SIZE;
@@ -171,11 +231,18 @@
 	sg_list_start = umem->sg_head.sgl;
 
 	while (npages) {
+
 		ret = get_user_pages(current, current->mm, cur_base,
 				     min_t(unsigned long, npages,
 					   PAGE_SIZE / sizeof (struct page *)),
 				     1, !umem->writable, page_list, vma_list);
 
+		if (ret == -EFAULT) /* may be a remapped area; try again */
+			ret = get_remap_pages(current->mm, cur_base,
+				     min_t(unsigned long, npages,
+					   PAGE_SIZE / sizeof (struct page *)),
+				     !umem->writable, page_list, vma_list);
+
 		if (ret < 0)
 			goto out;
 
@@ -227,7 +294,6 @@
 
 	return ret < 0 ? ERR_PTR(ret) : umem;
 }
-EXPORT_SYMBOL(ib_umem_get);
 
 static void ib_umem_account(struct work_struct *work)
 {
@@ -245,10 +311,10 @@
 }
 
 /**
- * ib_umem_release - release memory pinned with ib_umem_get
+ * ib_release_umem - release memory pinned with ib_umem_get
  * @umem: umem struct to release
  */
-void ib_umem_release(struct ib_umem *umem)
+void ib_release_umem(struct ib_umem *umem)
 {
 	struct ib_ucontext *context = umem->context;
 	struct mm_struct *mm;
@@ -298,9 +364,8 @@
 out:
 	kfree(umem);
 }
-EXPORT_SYMBOL(ib_umem_release);
 
-int ib_umem_page_count(struct ib_umem *umem)
+int ib_page_count_umem(struct ib_umem *umem)
 {
 	int shift;
 	int i;
@@ -315,4 +380,40 @@
 
 	return n;
 }
+
+struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,
+			    size_t size, int access, int dmasync)
+{
+	struct ib_umem_ops *ops = context->umem_ops;
+	struct ib_umem *umem;
+
+	umem = (ops && ops->get) ?
+		ops->get(context, addr, size, access, dmasync) :
+		ib_get_umem(context, addr, size, access, dmasync);
+
+	if (!IS_ERR(umem))
+		umem->context = context;
+
+	return umem;
+}
+EXPORT_SYMBOL(ib_umem_get);
+
+void ib_umem_release(struct ib_umem *umem)
+{
+	struct ib_umem_ops *ops = umem->context->umem_ops;
+
+	if (ops && ops->release)
+		ops->release(umem);
+	else
+		ib_release_umem(umem);
+}
+EXPORT_SYMBOL(ib_umem_release);
+
+int ib_umem_page_count(struct ib_umem *umem)
+{
+	struct ib_umem_ops *ops = umem->context->umem_ops;
+
+	return (ops && ops->page_count) ?
+		ops->page_count(umem) : ib_page_count_umem(umem);
+}
 EXPORT_SYMBOL(ib_umem_page_count);
diff -ruN a3/include/rdma/ib_verbs.h a4/include/rdma/ib_verbs.h
--- a3/include/rdma/ib_verbs.h	2015-09-10 09:32:01.420901195 -0700
+++ a4/include/rdma/ib_verbs.h	2015-09-10 09:32:16.723901528 -0700
@@ -1122,7 +1122,18 @@
 	u8	page_shift;
 };
 
+struct ib_ucontext;
+struct ib_umem_ops {
+	struct ib_umem	     *(*get)(struct ib_ucontext *context,
+				     unsigned long addr, size_t size,
+				     int access, int dmasync);
+	void		      (*release)(struct ib_umem *umem);
+	int		      (*page_count)(struct ib_umem *umem);
+};
+
 struct ib_ucontext {
+	struct ib_umem_ops     *umem_ops;	/* set to NULL for default ops */
+	void		       *umem_private_data;
 	struct ib_device       *device;
 	struct list_head	pd_list;
 	struct list_head	mr_list;
